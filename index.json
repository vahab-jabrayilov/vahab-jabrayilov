[{"authors":null,"categories":null,"content":"I am a first-year Ph.D. student in Computer Science at Columbia University advised by Kostis Kaffes. I am broadly interested in computer systems, especially cloud computing. I am looking for ways to implement and deploy systems for microsecond-scale tail latency across different layers of the stack.\nI received my bachelor’s degree at Middle East Technical University. You can reach me at vj2267 [at] columbia [dot] edu.\n","date":1689638400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1689638400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a first-year Ph.D. student in Computer Science at Columbia University advised by Kostis Kaffes. I am broadly interested in computer systems, especially cloud computing. I am looking for ways to implement and deploy systems for microsecond-scale tail latency across different layers of the stack.","tags":null,"title":"Vahab Jabrayilov","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://vjabrayilov.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Vahab Jabrayilov"],"categories":["Paper Reviews"],"content":"Review This paper is by the group of John Ousterhout and introduces the concept of so called “Granular computing”. They define “Granular computing” as the collection of very short-lived (10-100 usecs) tasks. The other main properties are large scale(1k-1M cooperating tasks) and short bursts(1-10 msecs of activity). Authors discuss the challenges coming with the new concept and present a few initial ideas about the required infrastructure to support it.\nIn granular computing, applications are composed of a very large number of small tasks running in microsecond scale both in parallel and sequentially, spread across thousands of machines. In addition, they are bursty, harnessing thousands of machines for just a few intense milliseconds of computation. In today’s software systems, overheads are too high to support small tasks efficiently or to scale up and down rapidly. Precisely, granular computing requires software infrastructure that operates at microsecond scale, but today’s systems are designed for millisecond scale.\nAuthors defend an extreme approach over an incremental one, which will stimulate innovative design thinking. They introduce real-time data-intensive processing and micro-lambdas as two classes of applications which can be possible by granular computing. It is argued that granular computing will enable real-time data-intensive processing to have smaller latency overhead for invoking requests and higher degree of concurrency. Regarding micro lambdas, it will support tasks three orders of magnitude smaller and will allow lambdas to work together by communicating.\nLow latency is one of the major challenges to overcome. Granular computing will require extremely low latency in task initiation and network communication. It shouldn’t take more than 1 usec to invoke a single task and 100 usecs to initiate tasks in the burst. Microsecond-scale network communication is essential both for fast burst startup and communication among short-lived tasks. Particularly, tail latency matters, since it is difficult to optimize and eliminate infrequent sources of overhead.\nExtreme bursts require to start very quickly by communicating with many servers in parallel and allocate resources on them. The second challenge here is resource utilization. In order to use resources efficiently, non-bursty background tasks must be run during the lulls between bursts, and the system must be able to preempt them very quickly at the start of the next burst.\nBelow mentioned overheads show why it is not possible to support granular computing with existing systems:\nCreating a thread on Linux takes 10 usecs A network RTT between two servers in the same datacenter can take 500 usecs or more when the network is loaded Spinning up a Linux container takes hundreds of milliseconds Initiating a job that spans a few hundred nodes with Spark, Kubernetes or AWS Lambda can take one second or more. Authors mention two major causes of these overheads. First, existing software infrastructure was designed to operate at a millisecond scale, not microsecond. The assumption of disk-based storage permeates much of the design of today’s software. A second problem is high layering. Each layer crossing adds overhead. Granular computing will require a significant re-architecture of the software stack to eliminate layers or bypass them.\nService concept is introduced as a granular computing unit, which can be either stateless or stateful. This separation based on state helps the underlying infrastructure to leverage its resources efficiently. They also propose two kinds of addressing that will raise the level of abstractions for network communication: service-based addressing and resource-based addressing. In service-based addressing, communication is directed at a particular service, and the infrastructure can load-balance the requests across the service’s instances. It is most appropriate for stateless services. In resource-based addressing, communication is directed to a particular resource, such as a piece of data with a particular data in a key-value store. It makes sense for stateful services. Authors also emphasize the need for efficient group communication with the higher-level addressing mechanisms they propose. Handling bursts are more complex and require sophisticated mechanisms to preempt resources from long running tasks, to isolate short-lived tasks efficiently. Moreover, keeping warm pools to reduce service latency would be unmanageable for large numbers of small tasks since they consume idle resources. Regarding persistence, they favor use of emerging nonvolatile memories or enforcing persistence only occasionally. But the latter isn’t trivial and may require assistance from applications.\nReferences Granular Computing ","date":1689638400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689638400,"objectID":"74ea8d78a3ad4d7192d97909a1e80679","permalink":"https://vjabrayilov.github.io/granularcomputing/","publishdate":"2023-07-18T00:00:00Z","relpermalink":"/granularcomputing/","section":"","summary":"A summary for the same paper from HotOS'19","tags":["serverless","granular computing","hotos","hotos19",2019],"title":"Granular Computing","type":"page"},{"authors":["Yigit Sever","Goktug Ekinci","Adnan Harun Dogan","Bugra Alparslan","Abdurrahman Said Gurbuz","Vahab Jabrayilov","Pelin Angin"],"categories":null,"content":" ","date":1663545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663545600,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"https://vjabrayilov.github.io/publication/conference-paper/","publishdate":"2022-09-19T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Microservices architecture has been praised as a lightweight, modular and robust alternative to monolithic software in recent years with software containerization bringing parallel ideas to the table against bare metal and even virtual machine based software deployment solutions. While containers provide support for agile software development in the cloud, they suffer from security issues due to their lightweight structure not providing isolation as strong as that of virtual machines. This calls for the development of robust intrusion detection systems (IDS) for containers, taking into account their specific vulnerabilities. Existing IDS for containerized software deployments have mainly used host-based syscall monitoring, with only a few utilizing network-based monitoring without justification for the particular sensor used. In this paper, we aim to close this research gap by empirically evaluating the performances of system call and network flow based features in machine learning-based intrusion detection for containers when subjected to the same attacks. Our results show that basing the IDS on the network layer exhibits better performance than the host-based IDS for the investigated vulnerabilities, demonstrating the need for network monitoring for enhanced container security.","tags":[],"title":"An Empirical Analysis of IDS Approaches in Container Securityr","type":"publication"},{"authors":["Vahab Jabrayilov"],"categories":["blog"],"content":"Overview The main goal was to support on the fly compaction of TSDB blocks without using any disk space or constant amount of the disk space. As analyzing, I came across with the following challenges:\nTSDB blocks have a special structure and the main challenge stems from already big size of a single block. compactor module is written such that it operates on the already downloaded blocks, so nothing to be done here. Initially, I had proposed to have a partial upload and download to accomodate a single download and upload problem, but I identified that downloading dependency (minio) already employs it. Analysis I moved to analyze the whole codebase, and with the help of my supervisor we identified BucketReader interface which is responsible for read access to an object storage bucket and all the other cloud provider support modules are implementing it. My supervisor suggested to imlement a new and scratch version of it which uses a streaming behavior. To be precise, it should bring chunks based on the read index and compact on chunk basis. Howevere, it is not a trivial thing and requires an in depth analysis of TSDB block structure and needs a planning mechanism wo bring which chunks and when to stop bringing. Since, the system is widely used in production, it should be well tested and shouldn;t result in data loss, hence increasing the complexity. So, we decided to take time to in depth analyze the above mentioned problems and come up with a strategy,\nBucketReader consists of Iter, Get, GetRange, Exists IsObjNotFoundErr and Attributes functions. In more detail,\nIter calls a given function in the bucket directory passing the entries to the function. Get provides a reader for the object GetRange provides a range reader similar to Get Exists checks whether the object exists in the bucket or not IsObjNotFoundErr returns true if object is not found in the bucket Attributes gives information about the specified object TSDB blocks consists of the head block and following N blocks. Each incoming time-series data point comes to the Head block and stays there, when all of them get old enough they are persisted in the disk as a separate block. WAL and mmap coordinates this process, but only the head and 1…N blocks are interested in case of compaction, since onlythey are resided in the object storage.\nEvery block has a unique identifier, called a Universally Unique Lexicographically Sortable Identifier(ULID). A block has 4 constituent parts:\nmeta.json –\u0026gt; the medatadata about the block chunks –\u0026gt; raw chunks index –\u0026gt; index of the block tombstones –\u0026gt; marker holding info whether the samples are deleted or not Only chunks is directory, other three are regular files.\nmeta.json has the following structure:\n{ \u0026#34;ulid\u0026#34;: \u0026#34;01EM6Q6A1YPX4G9TEB20J22B2R\u0026#34;, \u0026#34;minTime\u0026#34;: 1602237600000, \u0026#34;maxTime\u0026#34;: 1602244800000, \u0026#34;stats\u0026#34;: { \u0026#34;numSamples\u0026#34;: 553673232, \u0026#34;numSeries\u0026#34;: 1346066, \u0026#34;numChunks\u0026#34;: 4440437 }, \u0026#34;compaction\u0026#34;: { \u0026#34;level\u0026#34;: 1, \u0026#34;sources\u0026#34;: [ \u0026#34;01EM65SHSX4VARXBBHBF0M0FDS\u0026#34;, \u0026#34;01EM6GAJSYWSQQRDY782EA5ZPN\u0026#34; ] }, \u0026#34;version\u0026#34;: 1 } chunks directory has numbered files each representing a separate chunk and has a size of 512 MiB.\nPlanning phase of the compaction decides which blocks to compact. In our case, we should read the relevant block metadata and do planning based upon them. Since, metadata files are small in size reading several of them will not incur that much cost. With the new modified planning phase, we can decid upon which blocks are of interest for the current compaction stage and stream the chunks of those blocks in parallel. Reading those streams, one can compact and create new chunks in memory(it may also be mmap-ed file in disk). When the newly created chunks of the block reach some size it can be uploaded to object storage.\nUnfortunately, regarding time constraints and implementation complextiy, the result of above analysis is not completely ready. I plan to incremenetally implement by learning best practices. By analyizng the Thanos code base I get the good grasp of best practices of Go programming language and learned lots of unknown features to me. I believe those will help me to implement the result of analysis.\nAcknowledgements To sum up, GSoC was a good experience to learn, develop and be a part of larger open source community. Despite the fact that project was challenging, I learned a lot about time series databases, cloud object storage, and Go programming language. I would like to especially thank Ben Ye for his constant support and inspiration, and all Thanos community. Moreover, I would like to express my gratitude to all the folks in Google making the Google Summer of Code and inspiring newbies in the open source.\n","date":1662595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662595200,"objectID":"9d27554a92ed23a2ac723c70ba653c4d","permalink":"https://vjabrayilov.github.io/post/gsoc/","publishdate":"2022-09-08T00:00:00Z","relpermalink":"/post/gsoc/","section":"post","summary":"My GSOC final report","tags":["gsoc","thanos"],"title":"GSoC Final Submission Thanos","type":"post"},{"authors":["Vahab Jabrayilov"],"categories":["Paper Reviews"],"content":"Review Authors have a security background and paper aims at utilizing serverless design patterns in security applications. They solely focus on AWS Lambda, and assume that all the responsibility to secure lambda execution lies on the cloud provider. However, customers are also responsible to secure their communication. They briefly describe six design patterns and how one can use them to develop a security oriented application.\nPeriodic Invocation Pattern represents the kind of models that invoke lambda functions periodically by using schedulers. Each function carries out a simple task and reports the execution results to notification channels.\nEvent-Driven Pattern is where a set of lambda functions subscribe to events from cloud resources. These events trigger the execution of the subscribed lambda function passing the necessary context. It minimizes the cost by invoking lambda functions only when an event occurs and functions scale automatically based on the number of events, providing a scalable design.\nData Transformation Pattern. The ETL(extract-transform-load) data processing pipelines usually requires three steps: 1) extract data from a data source, 2) transform data by using frameworks such as Apache Spark or Flink, 3) load the transformed data into a database. Using lambda architecture, data processing tasks can be implemented as lambda functions.\nData Streaming Pattern. A lambda function sits in the path of the data stream and functions either as an aggregator or data partitioner. For example, a lambda can separate an incoming data stream into multiple streams(partition) or merge several incoming streams into one large data stream(aggregation).\nState Machine Pattern enables building a complex, stateful procedure by coordinating a collection of discrete Lambda functions using a tool such as AWS Step Functions. It shouldn’t scale the entire pattern as tasks defined for each state can be scaled up/down individually, Furthermore, the state machine offers a try/catch mechanism so that different or the same functions can be invoked depending on the failure reason.\nBundled Pattern combines two or more of the previously described patterns together by easily passing events sequentially between them. It resembles UNIX pipelines, where each function is small, precise and does one thing, but true power comes from chaining these together.\nAuthors show the time to start a lambda function(50-100ms) as a primary disadvantage. But lower prices for short running functions and better scalability are advantages.\nIn later sections of the paper they describe a threat intelligence platform built upon the above mentioned design patterns, which is not interesting for our purpose.\nAuthors also discuss some restrictions of current lambda design and offer possible solutions. Time bound execution is highlighted as one of the resource constraints and it can be avoided by splitting the original task across the multiple executions, but it is not possible for all workloads. They describe a proper solution as to either increase the execution time or to automatically pass state between executions so that the task can continue in another execution with the previous state. Lambda also suffers from lack of computing power. They argue that it would be better to have CPU limits configurable and support for GPUs in AWS Lambda. Moreover, lack of event tracing is emphasized by the authors.\nReferences Go Serverless: Securing Cloud via Serverless Design Patterns ","date":1662422400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662422400,"objectID":"b25cf73bf8f59667d8cecc09f5536718","permalink":"https://vjabrayilov.github.io/post/serverlessdesignpatterns/","publishdate":"2022-09-06T00:00:00Z","relpermalink":"/post/serverlessdesignpatterns/","section":"post","summary":"A summary of the same paper published in HotCloud'18","tags":["serverless","design patterns","hotcloud","hotcloud18",2018],"title":"Go Serverless: Securing Cloud via Serverless Design Patterns","type":"post"},{"authors":["Vahab Jabrayilov"],"categories":["Paper Reviews"],"content":"Review It’s a very short read and one of the early works. The main topic of discussion is the shortcomings of the Linux kernel for the current serverless paradigm. They argue that native container abstraction is ill-suited to be run as a unit of execution for serverless. They show that bypassing the kernel with unikernels can yield at least a factor of 6 better latency and throughput. While the unit of execution in the cloud shrinks, the complexity of the kernel is growing. It’s hard to introduce new abstractions to the kernel as it takes a lot of development investment and the kernel and its community have an inertia towards fast development.\nThere are 3 potential approaches to accommodate the serverless in the cloud:\nModify or extend the kernel Bypass the kernel Replace the kernel Very little of the current complexity of the kernel is required for serverless. Two main requirements for the serverless are well isolation from the other tenants as well as host platform, and lambda actions must perform well. The first one removes the solely implementation on top of the native linux processes from the scene. The latter one has two main constituents, low latency and higher throughput. According to the authors, the target goal for the latency is to fetch and start any action under 100ms without any cached state. To cover the financial objectives, achievable throughput should be at least 125.23 actions per second(One can see a back of the envelope calculation in the paper; simply they do calculations based on the pricing of AWS Lambda and EC2).\nEnhancing the Linux kernel and its container-related capabilities is the favored approach by the industry due to the large investment and dependency on the kernel itself. Moreover, containers are the most popular configuration and existing unit of execution in the current systems. To introduce the support for containers, the Linux kernel underwent major changes to include new abstractions like cgroups and namespaces. But the next significant changes can take longer, because the complexity affects implementation time. It is either hard to implement something in the kernel, and/or too hard to optimize the relevant code paths in the kernel, and/or too hard to secure the kernel.\nThe second approach is to bypass the complexity of the kernel and add another layer from which actions(lambda’s) can be started. It can be either achieved on top of hypervisor or unikernel monitors. Bypassing the kernel is commonplace for data plane network services using hardware-level virtualization and software frameworks like DPDK.\nReplacing the kernel with a new and custom designed for serverless type workloads is the way authors emphasize. They serve following alternative design considerations for the new kernel: Non -preemptive scheduling Limited set of I/O related calls No IPC No process synchronization\nTo support their arguments, they do a very simple experiment, running a simple “Hello” example in 3 ways. The baseline is running “echo “Hello”” as a native process, to experiment the containerization they run a similar binary inside the most minimal container via runc, statically linking the binary, without all the overhead of container runtime(Not using docker, networking, and etc.). To show the effect of kernel bypass, a unikernel(ukvm) running Solo5/ukvm is used. Results support the above mentioned arguments.\nReferences Will Serverless End the Dominance of Linux in the Cloud? ","date":1662336e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662336e3,"objectID":"f7e5d9ba6ec16f2ae0df0e37ebf94413","permalink":"https://vjabrayilov.github.io/endoflinux/","publishdate":"2022-09-05T00:00:00Z","relpermalink":"/endoflinux/","section":"","summary":"A summar of the same paper from HotOS'17","tags":["serverless","linux","hotos","hotos17",2017],"title":"Will Serverless End the Dominance of Linux in the Cloud? HotOS’17","type":"page"},{"authors":["Vahab Jabrayilov"],"categories":["Paper Reviews"],"content":" Summary Background Virtualization Lambda Interesting points References Summary Firecracker is a high-performance virtualization solution built to run Amazon’s serverless applications securely and with minimal resources. It now does so at immense scale. Background Virtualization Initially, a separate VM per Lambda customer, but existing VM solutions required significant resources, hence resulting in non-optimal utilization. 2 types of hypervisors Type 1 directly integrated in the hardware Type 2 run an operating system on top of the hardware, then run the hypervisor on top of that operating system Linux has a hypervisor built into the kernel - Kernel Virtual Machine, arguably a Type 1 hypervisor. ols2007v1-pages-225-230.pdf\nvirtio (linux provided interface) allows the user space kernel components to interact with the host OS. Rather than passing all interactions with a guest kernel directly to the host kernel, some functions (particularly, device interactions) go from a guest kernel to a virtual machine monitor (a.k.a VMM) (a popular example: QEMU) bellard.pdf\nCons(-) of QEMU: significant amount of code ⇒ more code, more potential attack surface redundant functionality, that Lambda would never use: such as support for USB drivers So, crosvm is chosen (a VMM open-sourced by Google and developed for ChromeOS) Paravirtualized Devices in crosvm - a Performance Panacea for Modern VMs Lambda When a lambda is invoked, the ensuing HTTP request hits an AWS Load Balancer. 4 main components: Workers the component running lambda’s code each runs many MicroVMs in “slots” and other services schedule code to be run in the MicoVMs when a lambda is invoked Frontend entrance into the lambda system receives invoke requests and communicate with Worker Manager to determine where to run the lambda. then directly communicates with the Workers Worker Manager ensures that the same lambda is routed to the same set of Workers keep tracks of where a lambda has been scheduled previously Placement service makes scheduling decisions to assign a lambda invocation to a worker Lambda Worker Architecture Firecracker VM Shim process process inside of the VM that communicates with an external side car called the Micro Manager Micro Manager a sidecar communicating over TCP with a Shim process reports metadata received back to the Placement service can be called from the Frontend to invoke a specific function on the function completion, receives the response from the shim process passing back to the client as needed Interesting points Only IO performance was inferior, and they argue that causes are no flushing to disk and an implementation of block IOs which performs IO serially; async IO support with io_uring can resolve it, there is an issue about it in github. [Perf] Implement async IO for the block device using io_uring · Issue #1600 · firecracker-microvm/firecracker\nLord of the io_uring\nReferences blog post: Firecracker: Lightweight Virtualization for Serverless Applications paper: (https://www.usenix.org/conference/nsdi20/presentation/agache) ","date":1658361600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1658361600,"objectID":"b592a97a6220a2d830e6f8b00a9b350e","permalink":"https://vjabrayilov.github.io/firecracker/","publishdate":"2022-07-21T00:00:00Z","relpermalink":"/firecracker/","section":"","summary":"A paper review for the same paper from NSDI'20","tags":["serverless","firecracker","microvm","nsdi","nsdi20",2020],"title":"Firecracker: Lightweight Virtualization for Serverless Applications","type":"page"},{"authors":["Vahab Jabrayilov"],"categories":["blog"],"content":" Key idea: read the paper in up to 3 passes 1st pass gives the general idea 5-10 mins get a bird’s-eye view and decide whether to do any more passes or not Do the following carefully read the title, abstract and introduction read the section and sub-section headings, but ignore everything else read the conclusions glance over the references, mentally ticking off the already read ones at the end of this pass, five Cs should be answered Category: What type of paper is this? A measurement paper? An analysis of an existing system? A description of a research prototype? Context: Which other paper is it related to? Which theoretical bases were used to analyze the problem? Correctness: Do the assumptions appear to be valid? Contributions: What are the paper’s main contributions? Clarity: Is the paper well written? this pass is adequate for papers that aren’t in the research area, but someday prove relevant Most reviewers make only one pass, take care to choose coherent section and sub-section titles and write concise and comprehensive abstracts. If a reviewer cannot understand the gist after one pass, most probably will be rejected; if a reader cannot understand the higlights of the paper after 5 mins, the paper will likely never be read. 2nd pass helps to grasp the paper’s content, not the details read the paper with greater care, but ignore details such as proofs Do the following Look carefully at the figures, diagrams and other illustrations in the paper. Pay special attention to graphs. Are the axes properly labeled? Are results shown with error bars, so that conclusions are statistically significant? Remember to mark relevant unread references for further reading up to 1 hour should be able to summarize the main thrust of the paper if not understood at this point, choose: set the paper aside return to the paper later, perhaps after reading the background material persevere and go on to the 3rd pass 3rd pass helps to understand the paper in depth attempt to virtually re-implement the paper identify and challenge every assumption in every statement jot down ideas for future work 4 or 5 hours for beginners, an hour for an experienced reader should be able to reconstruct the entire structure of the paper from memory, identify the strong and weak points. References http://ccr.sigcomm.org/online/files/p83-keshavA.pdf ","date":1658188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1658188800,"objectID":"8d1ef055c018bed1338a72e91118a552","permalink":"https://vjabrayilov.github.io/post/howtoreadapaper/","publishdate":"2022-07-19T00:00:00Z","relpermalink":"/post/howtoreadapaper/","section":"post","summary":"A summary of the whitepaper from SIGCOMM","tags":["insight"],"title":"How to Read a Paper","type":"post"},{"authors":["Vahab Jabrayilov"],"categories":["blog"],"content":" RabbitMQ Kafka RabbitMQ Architecture Kafka Architecture Use cases References RabbitMQ supports : AMQP : Advanced Message Queuing Protocol MQTT : MQ Telemetry Protocol STOMP : Streaming Text Oriented Messaging Protocol is known as a hybrid broker uses smart broker/dumb consumer model Kafka provides higher throughput, built-in partitioning, replication, and inherent fault-tolerance There are 2 async messagin patterns :\nMessage Queue\na creating app sends a msg to queue. When the consuming app is ready, it just connects to the queue and retrieves the msg, removing it from the queue. several consuming apps can exist, each message is only consumed by one. Publish/Subscribe (pub/sub)\nallows producers to publish msg’s which can be consumed by multiple consumer. if consuming apps are interested, they just subscribe to a channel used when a msg or event must trigger several actions unlike the message queue, pub/sub assures that consuming apps rcv msg’s in the same order as messaging system received them. RabbitMQ Architecture consists of producers exchanges queues consumers queues msg |----\u0026gt; producer -----\u0026gt; exchange --|----\u0026gt; consumers |----\u0026gt; or other exchanges queue is a sequential data structure: producers add to the end consumers get data from the top FIFO msg exchange determines routing 4 exchange types: direct can directly target msg’s to a particular queue direct exchange fanout route msg’s to all available queues fanout exchange topic header Kafka Architecture consists of producers consumers clusters brokers topics partitions producer --------\u0026gt; cluster -------\u0026gt; consumer Use cases Rabbit MQ complex routing - route msg’s among miltiple consuming apps, such as in a microservice architecture legacy applications Kafka high-troughput activity tracking stream processing event sourcing log aggregation References https://medium.com/@mbhanuka/kafka-vs-rabitmq-3ae75abe9c80\n","date":1658102400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1658102400,"objectID":"c668f9a038f09d9e9a94f9c1594ebf38","permalink":"https://vjabrayilov.github.io/post/kafkavsrabbit/","publishdate":"2022-07-18T00:00:00Z","relpermalink":"/post/kafkavsrabbit/","section":"post","summary":"Architectural review of Kafka and RabbitMQ","tags":["rabbit-mq","kafka"],"title":"Kafka vs RabbitMQ","type":"post"},{"authors":["Vahab Jabrayilov"],"categories":["blog"],"content":"3 levels of enlightenment:\nLevel 0: The Newcomer a ton of materials to master reaction: Overwhelmed problem: lack of breadth Level 1: The Half-Expert weakness finding machines reaction: dismissal and destruction Level 2: Chaotic times … Level 3: Nirvana people are able to provide and receive criticism without making it personal far more productive References https://hackingdistributed.com/2017/05/04/stages-of-enlightenment/ ","date":1658102400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1658102400,"objectID":"c6bab42e09c5650a158a5875e524d66b","permalink":"https://vjabrayilov.github.io/post/lstechieenlightenment/","publishdate":"2022-07-18T00:00:00Z","relpermalink":"/post/lstechieenlightenment/","section":"post","summary":"A summary of the original post by Emin Gun Sirer","tags":["insight"],"title":"Levels Of Techie Enlightenment","type":"post"},{"authors":["Vahab Jabrayilov"],"categories":["Paper Reviews"],"content":"Introduction Interestingly, most features improving the efficiency or reliability are the main cause of metastable failures. Trigger causes the open system (with an unctrolled source of load) to enter a bad state persisting even after the removal of the trigger. Failures that are resolved when the trigger is removed are not metastable. Recovery requires a strong corrective push, e.g. rebooting or dramatically reducing the load. Lifecycle of a metastable failure: load rises trigger stable state ------------\u0026gt; vulnerable state --------\u0026gt; metastable state (still healthy) the vulnerable state is not an overloaded state; system can run for a long time here; but can get stuck in metastable state w/o any increase in the load. interestingly most production systems prefers the vulnerable state since it has higher efficiency than the stable state. Feedback loop sustains the failure until a corrective action is applied. Case studies Request Retries Results in work amplification Consider a web application with db. Initially, system operates normally while load is just below a certain threshold. After a temporary network outage occurs and restores, retries are sent and this surge overloads the db. Overloaded db timeouts the upcoming queries and system will remain in metastable state untill the load is significantly reduced or timeout(retry) policy of db query changed. Look-aside Cache If cache is lost in the vulnerable state, db will be pushed to overloaded state. Cache will remain empty, system is trapped in metastable failure state; low cache hit -\u0026gt; slow db response -\u0026gt; prevents filling the cache Slow Error Handling Error handling has its cost and it can be a significant cost Trigger causes the system to run out of the resources used by error handling code, and error handling will make the shortage more severe. ??? – to think Is the following considered? –\u0026gt; An error occurs(like wrong input), error handling code makes the resources to run out, normal operations are deprived of resources resulting in additional errors(exceptions), which draw the system to a metastable state. Link Imbalance a very interesting and solid example; both requiring network and application layer collaboration to fix a complex and well designed hashing algorithm routes the requests; but the cache miss of a single shard in the application level increases the requests to the db. There is a connection pool with MRU policy, each spike of misses reaarranges the connection pool so that highest latency links are at the top and they will be used; resulting in further congestion. Approaches to Handling Metastability Trigger vs Root Cause: Sustaining feedback loop is the root cause rather than the trigger itself. Different triggers can result in the same failure state; hence solution is to address the sustaining effect. Change of Policy during Overload: e.g., make some failing requests to succeed. Prioritization: e.g., give priority to retries requests rather than new ones. Stress tests TODO: read the Kraken paper Organizational incentives Fast Error Paths Outlier Hygiene: same root cause can manfiest earlier as latency outlier or a cluster of errrors. Autoscaling Research Directions 2 main goals: designing systems that avoid metastable failures while operating efficiently developing mechanisms to recover from metastable failures as quickly as possible in cases that cannot be avoided Try to weaken the strongest feedback loops, discover about characteristic metric, using them generate warning signs; Learning the hidden capacity of the underlying system can help to take preventive actions. References https://sigops.org/s/conferences/hotos/2021/papers/hotos21-s11-bronson.pdf ","date":1658102400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1658102400,"objectID":"34930656500ae28a5fbccc00ec671423","permalink":"https://vjabrayilov.github.io/post/metastablefailures/","publishdate":"2022-07-18T00:00:00Z","relpermalink":"/post/metastablefailures/","section":"post","summary":"A summary of the same paper published in HotOS'21","tags":["failures","metastability","hotos","hotos21",2021],"title":"Metastable Failures in Distributed Systems","type":"post"},{"authors":["Vahab Jabrayilov"],"categories":null,"content":" Create your slides in Markdown - click the Slides button to check out the example. Supplementary notes can be added here, including code, math, and images.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"https://vjabrayilov.github.io/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://vjabrayilov.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":" ","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://vjabrayilov.github.io/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"Key value store with MultiPaxos consensus algorithm implemented in C++, Java, Go and Rust.","tags":null,"title":"Replicated Store","type":"project"},{"authors":["Vahab Jabrayilov","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. Create your slides in Markdown - click the Slides button to check out the example. Supplementary notes can be added here, including code, math, and images.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://vjabrayilov.github.io/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"}]