<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>hotos | Vahab Jabrayilov</title><link>https://vjabrayilov.github.io/tag/hotos/</link><atom:link href="https://vjabrayilov.github.io/tag/hotos/index.xml" rel="self" type="application/rss+xml"/><description>hotos</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 18 Jul 2023 00:00:00 +0000</lastBuildDate><image><url>https://vjabrayilov.github.io/media/icon_hufdc9ffe2c12a5bb5ad2cfdfce17ddfaa_56068_512x512_fill_lanczos_center_3.png</url><title>hotos</title><link>https://vjabrayilov.github.io/tag/hotos/</link></image><item><title>Granular Computing</title><link>https://vjabrayilov.github.io/granularcomputing/</link><pubDate>Tue, 18 Jul 2023 00:00:00 +0000</pubDate><guid>https://vjabrayilov.github.io/granularcomputing/</guid><description>&lt;h2 id="review">Review&lt;/h2>
&lt;p>This paper is by the group of John Ousterhout and introduces the concept of so called “Granular computing”. They define “Granular computing” as the collection of very short-lived (10-100 usecs) tasks. The other main properties are large scale(1k-1M cooperating tasks) and short bursts(1-10 msecs of activity). Authors discuss the challenges coming with the new concept and present a few initial ideas about the required infrastructure to support it.&lt;/p>
&lt;p>In granular computing, applications are composed of a very large number of small tasks running in microsecond scale both in parallel and sequentially, spread across thousands of machines. In addition, they are bursty, harnessing thousands of machines for just a few intense milliseconds of computation. In today’s software systems, overheads are too high to support small tasks efficiently or to scale up and down rapidly. Precisely, granular computing requires software infrastructure that operates at microsecond scale, but today’s systems are designed for millisecond scale.&lt;/p>
&lt;p>Authors defend an extreme approach over an incremental one, which will stimulate innovative design thinking. They introduce real-time data-intensive processing and micro-lambdas as two classes of applications which can be possible by granular computing. It is argued that granular computing will enable real-time data-intensive processing to have smaller latency overhead for invoking requests and higher degree of concurrency. Regarding micro lambdas, it will support tasks three orders of magnitude smaller and will allow lambdas to work together by communicating.&lt;/p>
&lt;p>Low latency is one of the major challenges to overcome. Granular computing will require extremely low latency in task initiation and network communication. It shouldn’t take more than 1 usec to invoke a single task and 100 usecs to initiate tasks in the burst. Microsecond-scale network communication is essential both for fast burst startup and communication among short-lived tasks. Particularly, tail latency matters, since it is difficult to optimize and eliminate infrequent sources of overhead.&lt;/p>
&lt;p>Extreme bursts require to start very quickly by communicating with many servers in parallel and allocate resources on them. The second challenge here is resource utilization. In order to use resources efficiently, non-bursty background tasks must be run during the lulls between bursts, and the system must be able to preempt them very quickly at the start of the next burst.&lt;/p>
&lt;p>Below mentioned overheads show why it is not possible to support granular computing with existing systems:&lt;/p>
&lt;ul>
&lt;li>Creating a thread on Linux takes 10 usecs&lt;/li>
&lt;li>A network RTT between two servers in the same datacenter can take 500 usecs or more when the network is loaded&lt;/li>
&lt;li>Spinning up a Linux container takes hundreds of milliseconds&lt;/li>
&lt;li>Initiating a job that spans a few hundred nodes with Spark, Kubernetes or AWS Lambda can take one second or more.&lt;/li>
&lt;/ul>
&lt;p>Authors mention two major causes of these overheads. First, existing software infrastructure was designed to operate at a millisecond scale, not microsecond. The assumption of disk-based storage permeates much of the design of today’s software. A second problem is high layering. Each layer crossing adds overhead. Granular computing will require a significant re-architecture of the software stack to eliminate layers or bypass them.&lt;/p>
&lt;p>Service concept is introduced as a granular computing unit, which can be either stateless or stateful. This separation based on state helps the underlying infrastructure to leverage its resources efficiently. They also propose two kinds of addressing that will raise the level of abstractions for network communication: service-based addressing and resource-based addressing. In service-based addressing, communication is directed at a particular service, and the infrastructure can load-balance the requests across the service’s instances. It is most appropriate for stateless services. In resource-based addressing, communication is directed to a particular resource, such as a piece of data with a particular data in a key-value store. It makes sense for stateful services. Authors also emphasize the need for efficient group communication with the higher-level addressing mechanisms they propose. Handling bursts are more complex and require sophisticated mechanisms to preempt resources from long running tasks, to isolate short-lived tasks efficiently. Moreover, keeping warm pools to reduce service latency would be unmanageable for large numbers of small tasks since they consume idle resources. Regarding persistence, they favor use of emerging nonvolatile memories or enforcing persistence only occasionally. But the latter isn’t trivial and may require assistance from applications.&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://dl.acm.org/doi/pdf/10.1145/3317550.3321447" target="_blank" rel="noopener">Granular Computing&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Metastable Failures in Distributed Systems</title><link>https://vjabrayilov.github.io/post/metastablefailures/</link><pubDate>Mon, 18 Jul 2022 00:00:00 +0000</pubDate><guid>https://vjabrayilov.github.io/post/metastablefailures/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;ul>
&lt;li>Interestingly, most features improving the efficiency or reliability are the main cause of &lt;em>metastable failures&lt;/em>.&lt;/li>
&lt;li>Trigger causes the open system (with an unctrolled source of load) to enter a &lt;strong>bad state&lt;/strong> persisting even after the removal of the trigger.&lt;/li>
&lt;li>Failures that are &lt;strong>resolved when the trigger is removed&lt;/strong> are not metastable.&lt;/li>
&lt;li>Recovery requires &lt;em>a strong corrective push&lt;/em>, e.g. rebooting or dramatically reducing the load.&lt;/li>
&lt;li>Lifecycle of a metastable failure:
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl"> load rises trigger
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> stable state ------------&amp;gt; vulnerable state --------&amp;gt; metastable state
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> (still healthy)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>the vulnerable state is not an overloaded state; system can run for a long time here; but can get stuck in &lt;em>metastable state&lt;/em> w/o any increase in the load.&lt;/li>
&lt;li>interestingly most production systems prefers the vulnerable state since it has higher efficiency than the stable state.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;em>Feedback loop&lt;/em> sustains the failure until a corrective action is applied.&lt;/li>
&lt;/ul>
&lt;h2 id="case-studies">Case studies&lt;/h2>
&lt;ul>
&lt;li>Request Retries
&lt;ul>
&lt;li>Results in work amplification&lt;/li>
&lt;li>Consider a web application with db. Initially, system operates normally while load is just below a certain threshold. After a temporary network outage occurs and restores, retries are sent and this surge overloads the db. Overloaded db timeouts the upcoming queries and system will remain in metastable state untill the load is significantly reduced or timeout(retry) policy of db query changed.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Look-aside Cache
&lt;ul>
&lt;li>If cache is lost in the vulnerable state, db will be pushed to overloaded state. Cache will remain empty, system is trapped in metastable failure state;
&lt;ul>
&lt;li>low cache hit -&amp;gt; slow db response -&amp;gt; prevents filling the cache&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Slow Error Handling
&lt;ul>
&lt;li>Error handling has its cost and it can be a significant cost&lt;/li>
&lt;li>Trigger causes the system to run out of the resources used by error handling code, and error handling will make the shortage more severe.&lt;/li>
&lt;li>&lt;strong>??? &amp;ndash; to think&lt;/strong> Is the following considered? &amp;ndash;&amp;gt; An error occurs(like wrong input), error handling code makes the resources to run out, normal operations are deprived of resources resulting in additional errors(exceptions), which draw the system to a metastable state.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Link Imbalance
&lt;ul>
&lt;li>a very interesting and solid example; both requiring network and application layer collaboration to fix&lt;/li>
&lt;li>a complex and well designed hashing algorithm routes the requests; but the cache miss of a single shard in the application level increases the requests to the db. There is a connection pool with MRU policy, each spike of misses reaarranges the connection pool so that highest latency links are at the top and they will be used; resulting in further congestion.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="approaches-to-handling-metastability">Approaches to Handling Metastability&lt;/h2>
&lt;ul>
&lt;li>Trigger vs Root Cause: Sustaining feedback loop is the root cause rather than the trigger itself. Different triggers can result in the same failure state; hence solution is to address the sustaining effect.&lt;/li>
&lt;li>Change of Policy during Overload: e.g., make some failing requests to succeed.&lt;/li>
&lt;li>Prioritization: e.g., give priority to retries requests rather than new ones.&lt;/li>
&lt;li>Stress tests &lt;strong>TODO: read the Kraken paper&lt;/strong>&lt;/li>
&lt;li>Organizational incentives&lt;/li>
&lt;li>Fast Error Paths&lt;/li>
&lt;li>Outlier Hygiene: same root cause can manfiest earlier as latency outlier or a cluster of errrors.&lt;/li>
&lt;li>Autoscaling&lt;/li>
&lt;/ul>
&lt;h2 id="research-directions">Research Directions&lt;/h2>
&lt;ul>
&lt;li>2 main goals:
&lt;ul>
&lt;li>designing systems that avoid metastable failures while operating efficiently&lt;/li>
&lt;li>developing mechanisms to recover from metastable failures as quickly as possible in cases that cannot be avoided&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Try to weaken the strongest feedback loops, discover about characteristic metric, using them generate warning signs;&lt;/li>
&lt;li>Learning the hidden capacity of the underlying system can help to take preventive actions.&lt;/li>
&lt;/ul>
&lt;h2 id="references">References&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://sigops.org/s/conferences/hotos/2021/papers/hotos21-s11-bronson.pdf" target="_blank" rel="noopener">https://sigops.org/s/conferences/hotos/2021/papers/hotos21-s11-bronson.pdf&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>